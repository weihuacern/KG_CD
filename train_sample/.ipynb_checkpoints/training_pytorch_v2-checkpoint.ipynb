{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "75d61c38-954c-4bd1-a127-5d17e1812402",
    "_uuid": "2ae1f395d8768512da0739dded661e329dbfa14e"
   },
   "source": [
    "This notebook contains a generator class for Keras called `BSONIterator` that can read directly from the BSON data. You can use it in combination with `ImageDataGenerator` for doing data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "4036f11e-5223-46e5-8948-5cbf70a9bf73",
    "_uuid": "3767d2738682e30c292a466f66bc75fcc80a5076"
   },
   "outputs": [],
   "source": [
    "import os, sys, math, io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import bson\n",
    "import struct\n",
    "from PIL import Image\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4212f62a-86c3-4a13-8882-b4c90f64dac5",
    "_uuid": "8a4bd65c90576d611340e36bce5f49896d942ed1"
   },
   "source": [
    "# Part 2: The generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1da50467-d11d-417e-9367-bd8574dfe61a",
    "_uuid": "55331a57195739250c5b530160cf32a57b9a2464"
   },
   "source": [
    "First load the lookup tables from the CSV files (you don't need to do this if you just did all the steps from part 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "19c24f98-551d-4b95-b8f7-8497ab09eb56",
    "_uuid": "0c0dadb16f6b094748e967e9c682bb18b3b5c336"
   },
   "outputs": [],
   "source": [
    "train_offsets_df = pd.read_csv(\"train_offsets.csv\", index_col=0)\n",
    "train_images_df = pd.read_csv(\"train_images.csv\", index_col=0)\n",
    "val_images_df = pd.read_csv(\"val_images.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "17cb7bb4-4941-4d0d-8fa5-2dd60b7014ce",
    "_uuid": "edbb272356d65e3ec9b540bd4597464ae4afa3c5"
   },
   "source": [
    "The Keras generator is implemented by the `BSONIterator` class. It creates batches of images (and their one-hot encoded labels) directly from the BSON file. It can be used with multiple workers.\n",
    "\n",
    "**Note:** For fastest results, put the train.bson and test.bson files on a fast drive (SSD).\n",
    "\n",
    "See also the code in: https://github.com/fchollet/keras/blob/master/keras/preprocessing/image.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "78d8b565-336e-4836-b613-768bb6581499",
    "_uuid": "c43d82c645b098b2dd8fc0962bd392bdfc43057d"
   },
   "outputs": [],
   "source": [
    "class BSONIterator(Dataset):\n",
    "    def __init__(self, bson_file, images_df, offsets_df, transform, lock, train=True):\n",
    "        super(BSONIterator, self).__init__()\n",
    "        self.file = bson_file\n",
    "        self.images_df = images_df\n",
    "        self.offsets_df = offsets_df\n",
    "        self.transform = transform\n",
    "        self.lock = lock\n",
    "        self.train = train\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Protect file and dataframe access with a lock.\n",
    "#         with self.lock:\n",
    "        image_row = self.images_df.iloc[idx]\n",
    "        product_id = image_row[\"product_id\"]\n",
    "        offset_row = self.offsets_df.loc[product_id]\n",
    "        # Random access this product's data from the BSON file.\n",
    "        self.file.seek(offset_row[\"offset\"])\n",
    "        item_data = self.file.read(offset_row[\"length\"])\n",
    "        # Grab the image from the product.\n",
    "        item = bson.BSON.decode(item_data)\n",
    "        img_idx = image_row[\"img_idx\"]\n",
    "        bson_img = item[\"imgs\"][img_idx][\"picture\"]\n",
    "\n",
    "        # Load the image.\n",
    "        image = io.BytesIO(bson_img)\n",
    "        img = Image.open(image)\n",
    "        x = self.transform(img)\n",
    "        if self.train:\n",
    "            y = image_row[\"category_idx\"]\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "86525843-13cc-413e-b372-085e4864a45a",
    "_uuid": "a1de21a39914a95128adb7675b5fa17a5284f338"
   },
   "outputs": [],
   "source": [
    "data_dir = \"./input/\"\n",
    "# file_dir = r'C:\\Users\\YANG\\Downloads\\cdiscount'\n",
    "train_bson_path = os.path.join(data_dir, \"train_example.bson\")\n",
    "train_bson_file = open(train_bson_path, \"rb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "41fe73d402d56528a0a8385ed73b089dcdd5c446"
   },
   "source": [
    "Because the training and validation generators read from the same BSON file, they need to use the same lock to protect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a4533712770944483a963eea135a8d279cc1f5b9"
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "# transform_train = T.Compose([T.CenterCrop(size=160), T.RandomHorizontalFlip(), T.ToTensor(),\n",
    "#                              T.Normalize(mean=mean, std=std)])\n",
    "# transform_val = T.Compose([T.CenterCrop(size=160), T.ToTensor(),T.Normalize(mean=mean, std=std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "transform_train = T.Compose([T.RandomHorizontalFlip(), \n",
    "                             T.ToTensor(),T.Normalize(mean=mean, std=std)])\n",
    "transform_val = T.Compose([T.ToTensor(),T.Normalize(mean=mean, std=std)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2742b451-159b-4353-a7eb-96885fdf8919",
    "_uuid": "a4a76294778c99600228091e797507026d8ba4c3"
   },
   "source": [
    "Create a generator for training and a generator for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d9381773-ad58-4656-b225-306e68a603f7",
    "_uuid": "ffc317a85c6849dca92c8e5322eecff66269cca4"
   },
   "outputs": [],
   "source": [
    "train_gen = BSONIterator(train_bson_file, train_images_df, train_offsets_df, transform_train, lock, train = True)\n",
    "val_gen = BSONIterator(train_bson_file, val_images_df, train_offsets_df, transform_val, lock, train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bx, by = train_gen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_gen), len(val_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 102\n",
    "loader_train = DataLoader(train_gen, batch_size=batch_size, num_workers=4, pin_memory = False)\n",
    "# loader_val = DataLoader(val_gen, batch_size=batch_size, \n",
    "#                           sampler=sampler.RandomSampler(val_gen), num_workers=4, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(loader_train), len(loader_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5c5871a3-6d4a-4260-b8d4-462454ad8983",
    "_uuid": "c3cb7cadf70e26f22f9cd35f8f375b4e9483325a"
   },
   "source": [
    "## How fast is the generator? Create a single batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = iter(loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bx, by = next(itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itr = iter(loader_train)\n",
    "start = time.time()\n",
    "bx, by = next(itr)\n",
    "end = time.time()\n",
    "print(bx.size(), by.size())\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2250dba6-6ab0-4417-854e-6a3eb924fc33",
    "_uuid": "3daa4847924e17f4bab9a58470460667dc517075"
   },
   "source": [
    "# Part 3: Training\n",
    "\n",
    "Create a very Resnet18 model and train it, to test that the generators work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ResNet18**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.resnet18(pretrained=True)\n",
    "# model.avgpool = nn.AvgPool2d(kernel_size = 5)\n",
    "# model.fc = nn.Linear(in_features=512, out_features=5270) #or 49 + 483 + 5270\n",
    "# for layer in [model.conv1, model.bn1, model.relu, model.maxpool, model.layer1, model.layer2]:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.resnet18(pretrained=True)\n",
    "# model.fc = nn.Linear(in_features=512, out_features=5270) #or 49 + 483 + 5270\n",
    "# for layer in [model.conv1, model.bn1, model.relu, model.maxpool, model.layer1, model.layer2]:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "model.avgpool = nn.AvgPool2d(kernel_size = 6)\n",
    "model.fc = nn.Linear(in_features=512, out_features=5270) #or 49 + 483 + 5270\n",
    "for layer in [model.conv1, model.bn1, model.relu, model.maxpool, model.layer1, model.layer2]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ResNet50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.resnet50(pretrained=True)\n",
    "# model.avgpool = nn.AvgPool2d(kernel_size = 5)\n",
    "# model.fc = nn.Linear(in_features=2048, out_features=5270) #or 49 + 483 + 5270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResnetAffine(nn.Module):\n",
    "#     def __init__(self, resnet, in_features, out_features):\n",
    "#         super(ResnetAffine, self).__init__()\n",
    "#         self.resnet = resnet\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.affine = nn.Linear(in_features=in_features, out_features=out_features)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.resnet(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.affine(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet = torchvision.models.resnet18(pretrained=True)\n",
    "# resnet.avgpool = nn.AvgPool2d(kernel_size = 5)\n",
    "# for param in resnet.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in resnet.fc.parameters():\n",
    "#     param.requires_grad = True\n",
    "# model = ResnetAffine(resnet, 1000, 5270)\n",
    "# model.affine.bias.data.zero_()\n",
    "# init.kaiming_uniform(model.affine.weight.data)\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init.kaiming_normal(model.fc.weight.data)\n",
    "model.fc.bias.data.zero_()\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(lr, optimizer, epoch, denominator = 2):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = lr * (0.1 ** (epoch // denominator))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.max(dim=1)\n",
    "    correct = pred.eq(target)\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct.float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "# def accuracy(output, target, topk=(1,)):\n",
    "#     \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "#     maxk = max(topk)\n",
    "#     batch_size = target.size(0)\n",
    "\n",
    "#     _, pred = output.topk(maxk, 1, True, True)\n",
    "#     pred = pred.t()\n",
    "#     correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "#     res = []\n",
    "#     for k in topk:\n",
    "#         correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "#         res.append(correct_k.mul_(100.0 / batch_size))\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, print_freq = 50):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    loss_log = []\n",
    "    acc_log = []\n",
    "#     top5 = AverageMeter() #only need top1\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (img, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda(async=True)\n",
    "        img = img.cuda(async=True)\n",
    "        img_var = Variable(img)\n",
    "        target_var = Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output = model(img_var)\n",
    "        loss = criterion(output, target_var)\n",
    "        loss_log.append(loss)\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1, ))[0]#only need top1\n",
    "        losses.update(loss.data[0], img.size(0)) #[0] to take out the float inside torch.Tensor\n",
    "        top1.update(prec1[0], img.size(0))\n",
    "        acc_log.append(top1.val)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "    return loss_log, acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, print_freq=50):\n",
    "    batch_time = AverageMeter()\n",
    "#     losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "#     top5 = AverageMeter() #only need top1\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (img, target) in enumerate(val_loader):\n",
    "        \n",
    "        target = target.cuda(async=True)\n",
    "        img = img.cuda(async=True)\n",
    "        img_var = Variable(img, volatile=True)\n",
    "        target_var = Variable(target, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(img_var)\n",
    "#         loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]#only need top1\n",
    "#         losses.update(loss.data[0], img.size(0))\n",
    "        top1.update(prec1[0], img.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, top1=top1))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    best_prec1 = 46.5\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr = 1e-2, momentum=0.9, \n",
    "                          weight_decay=0)\n",
    "    resume = None\n",
    "    start_epoch = 0\n",
    "    epochs = 1\n",
    "    arch = 'resnet18'\n",
    "\n",
    "    if resume:\n",
    "        if os.path.isfile(resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "            checkpoint = torch.load(resume)\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "    #     if args.distributed:\n",
    "    #         train_sampler.set_epoch(epoch)\n",
    "        adjust_learning_rate(lr=1e-2, optimizer=optimizer, epoch=epoch, denominator=2)\n",
    "\n",
    "        # train for one epoch\n",
    "        loss_log, acc_log = train(train_loader=loader_train, model=model, criterion=criterion,\n",
    "                                  optimizer=optimizer, epoch=epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader=loader_val, model=model)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': arch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "\n",
    "        #plot loss and acc\n",
    "        fig = plt.figure(figsize = (6,3), dpi = 1200)\n",
    "        loss_log = np.array(list(map(lambda x: x.data[0], loss_log)))\n",
    "        ax1 = plt.subplot(121)\n",
    "        ax1.plot(loss_log)\n",
    "        ax1.set_ylabel('Loss', weight = 'bold')\n",
    "        acc_log = np.array(acc_log)\n",
    "        ax2 = plt.subplot(122)\n",
    "        ax2.plot(acc_log)\n",
    "        ax2.set_ylabel('Train_accuracy', weight = 'bold')\n",
    "        np.savetxt(X=np.vstack((loss_log, acc_log)), fname='loss_acc_log.txt', fmt='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2116a8c3-ec60-4878-93af-94c185c08cb3",
    "_uuid": "ee05e14fd2e9354008f23941f97ee3ff7ff73aaf"
   },
   "source": [
    "# Part 4: Test set predictions\n",
    "\n",
    "Note: The previous version of this kernel used `BSONIterator` to load the test set images in batches. However, storing the prediction results takes up a huge amount of memory. \n",
    "\n",
    "I suggest using a different kind of generator instead, something like the following:\n",
    "\n",
    "```\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "submission_df = pd.read_csv(data_dir + \"sample_submission.csv\")\n",
    "submission_df.head()\n",
    "\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "data = bson.decode_file_iter(open(test_bson_path, \"rb\"))\n",
    "\n",
    "with tqdm(total=num_test_products) as pbar:\n",
    "    for c, d in enumerate(data):\n",
    "        product_id = d[\"_id\"]\n",
    "        num_imgs = len(d[\"imgs\"])\n",
    "\n",
    "        batch_x = np.zeros((num_imgs, 180, 180, 3), dtype=K.floatx())\n",
    "\n",
    "        for i in range(num_imgs):\n",
    "            bson_img = d[\"imgs\"][i][\"picture\"]\n",
    "\n",
    "            # Load and preprocess the image.\n",
    "            img = load_img(io.BytesIO(bson_img), target_size=(180, 180))\n",
    "            x = img_to_array(img)\n",
    "            x = test_datagen.random_transform(x)\n",
    "            x = test_datagen.standardize(x)\n",
    "\n",
    "            # Add the image to the batch.\n",
    "            batch_x[i] = x\n",
    "\n",
    "        prediction = model.predict(batch_x, batch_size=num_imgs)\n",
    "        avg_pred = prediction.mean(axis=0)\n",
    "        cat_idx = np.argmax(avg_pred)\n",
    "        \n",
    "        submission_df.iloc[c][\"category_id\"] = idx2cat[cat_idx]        \n",
    "        pbar.update()\n",
    "        \n",
    "submission_df.to_csv(\"my_submission.csv.gz\", compression=\"gzip\", index=False)        \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cbfc7694-29b3-4197-b2dd-daca30ed7468",
    "_uuid": "63f279cfa4548b449061ab932e15005d1ee59e26"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
